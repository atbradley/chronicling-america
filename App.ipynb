{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring _Chronicling America_\n",
    "\n",
    "Enter a search below and wait. Older newspapers tend to take more time to process--they have fewer pictures, so more text (They also don't use modern spelling, so the spellchecker spends more time on them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, namedtuple\n",
    "from difflib import HtmlDiff\n",
    "import io\n",
    "from math import ceil\n",
    "from operator import attrgetter, itemgetter\n",
    "from pathlib import Path\n",
    "import re\n",
    "from tempfile import NamedTemporaryFile\n",
    "from time import sleep\n",
    "from urllib.parse import quote, urlencode\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster\n",
    "import ipywidgets as widgets\n",
    "import nltk\n",
    "from nltk.corpus import brown, stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from PIL import Image\n",
    "from pydash import py_\n",
    "from requests import get, post\n",
    "import spacy\n",
    "\n",
    "import bingsc_settings as sc\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BMAPSKEY = Path('./bingMapsKey').open('r').read().strip()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "sws = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get word frequencies from the Brown Corpus.\n",
    "brownfreqs = Counter(word.lower() for word in brown.words())\n",
    "brownlen = len(brown.words())\n",
    "brownprobs = {k: v/brownlen for k, v in brownfreqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entity = namedtuple(\"Entity\", ['text', 'lemma', 'stem', \n",
    "                              'first_appearance', 'kind', 'count'])\n",
    "EntityData = namedtuple('EntityData', ('name', 'description', 'url', 'lat', 'long'))\n",
    "Paper = namedtuple(\"Paper\", ['title', 'place', 'coords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "statusWidget = widgets.Output(layout={'border': '2px solid blue'})\n",
    "\n",
    "progressbar = widgets.IntProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=1,\n",
    "    step=1,\n",
    "    description='Loading:',\n",
    "    bar_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "urlbox = widgets.Output(layout={'border': '1px solid black', 'grid_area': 'header'})\n",
    "entitiesbox = widgets.Output(layout={'border': '1px solid black', 'grid_area': 'entities'})\n",
    "nlpbox = widgets.Output(layout={'border': '1px solid black', 'grid_area': 'nlp'})\n",
    "mapbox  = widgets.Output(layout={'border': '1px solid black', 'grid_area': 'map'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geocodecity(citystate):\n",
    "    try:\n",
    "        (city, state) = citystate.split(',')\n",
    "    except:\n",
    "        return False\n",
    "    url = f\"http://dev.virtualearth.net/REST/v1/Locations?countryRegion=US\\\n",
    "&adminDistrict={state}&locality={city}&includeNeighborhood=true&\\\n",
    "maxResults=1&key={BMAPSKEY}\"\n",
    "    \n",
    "    return py_.get(get(url).json(), 'resourceSets.0.resources.0.point.coordinates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC_MAX_LENGTH = 10000\n",
    "\n",
    "def bing_spellcheck(text):\n",
    "    \"\"\"\n",
    "    Send `text` to the Bing Spell Check API, breaking it into\n",
    "    chunks if necessary to stay below the maximum input length of \n",
    "    10000 bytes.\n",
    "    \n",
    "    Returns corrected text and the raw response from Bing.\n",
    "    \"\"\"\n",
    "    #Many (most? all?) of these documents seem to contain \n",
    "    #some characters that breaks the spell checker. \n",
    "    #This regex replaces anything but letters, numbers, \n",
    "    #and common punctuation with spaces.\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\.\\?!,\\' ]+', ' ', text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    bit_length = ceil(len(sentences) / (ceil(len(text) / SC_MAX_LENGTH)))\n",
    "\n",
    "    sctext = ''\n",
    "    scoutp = []\n",
    "    \n",
    "    scparams = {\n",
    "        'mkt':'en-us',\n",
    "        'mode':'proof'\n",
    "    }\n",
    "    scheaders = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'Ocp-Apim-Subscription-Key': sc.KEY1,\n",
    "        #'X-Search-Location': 'lat:41.823611;long:-71.422222;re:2000'\n",
    "    }\n",
    "    \n",
    "    while len(sentences) > 0:\n",
    "        bit = False\n",
    "        mult = 1.1\n",
    "        diff = 0\n",
    "        while (bit is False or len(bit) > SC_MAX_LENGTH) and mult > .1:\n",
    "            mult -= .1\n",
    "            scount = ceil(bit_length*mult)\n",
    "            bit = ' '.join(sentences[:scount])         \n",
    "            scbit = ' '.join(sentences[:scount])\n",
    "        \n",
    "        del(sentences[:scount])\n",
    "            \n",
    "        \n",
    "        data = {\n",
    "            'text': bit\n",
    "        }\n",
    "        rsp = post(sc.ENDPOINT, headers=scheaders, params=scparams, data=data)\n",
    "        scdata = rsp.json()\n",
    "        \n",
    "        for repl in scdata.get('flaggedTokens', []):\n",
    "            start = repl['offset'] - diff\n",
    "            token = repl['token']\n",
    "            end = start + len(token)\n",
    "            \n",
    "            sug = repl['suggestions'][0].get('suggestion', token)\n",
    "            \n",
    "            #print('token:', token, 'suggestion:', sug)\n",
    "            #print(scbit[start-100:start], '----', scbit[start:end], '----', scbit[end:end+100])\n",
    "            scbit = scbit[:start] + sug + scbit[end:]\n",
    "            diff += len(token) - len(sug)\n",
    "            \n",
    "        sctext += scbit\n",
    "        scoutp.append(scdata)\n",
    "            \n",
    "    return sctext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchCA(year1:int, year2:int, searchterm:str, count:int):\n",
    "    \"\"\"Search Chronicling America\"\"\"\n",
    "    #URL template for searching Chronicling America.\n",
    "    searchurl = 'https://chroniclingamerica.loc.gov/search/pages/results?'\n",
    "    \n",
    "    urlbox.clear_output()\n",
    "    \n",
    "    if year1 > year2:\n",
    "        startyear = year2\n",
    "        endyear = year1\n",
    "    else:\n",
    "        startyear = year1\n",
    "        endyear = year2\n",
    "        \n",
    "    searchterms = {\n",
    "        'searchType': 'basic',\n",
    "        'dateFilterType': 'yearRange',\n",
    "        'language': 'eng',\n",
    "        'proxtext': quote(searchterm), \n",
    "        'date1': startyear, \n",
    "        'date2': endyear,\n",
    "        'rows': count,\n",
    "    }\n",
    "    \n",
    "    url = searchurl + urlencode(searchterms)\n",
    "    with urlbox:\n",
    "        display(HTML(f'<a href=\"{url}\" target=\"_blank\">{url}</a>'))\n",
    "    \n",
    "    searchterms['format'] = 'json'\n",
    "    papersearch = get(searchurl + urlencode(searchterms)).json()\n",
    "    return papersearch['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntities(page_data:dict):\n",
    "    text = page_data['ocr_eng']\n",
    "    sctext = bing_spellcheck(text)\n",
    "    scytext = nlp(sctext)\n",
    "    \n",
    "    #Get named entities from spacy and remove really short ones and \n",
    "    #uninteresting tags.\n",
    "    entities = [Entity(ent.text, ent.lemma_, stemmer.stem(ent.text), \n",
    "                         ent.start_char, ent.label_, 1) \n",
    "         for ent in scytext.ents\n",
    "         if ent.label_ not in ('DATE', 'MONEY', 'CARDINAL', 'ORDINAL')\n",
    "               and len(ent.text) > 3\n",
    "         ]\n",
    "    \n",
    "    for e in range(len(entities)):\n",
    "        if entities[e] != False:\n",
    "            for f in range(e+1, len(entities)):\n",
    "                if entities[f] != False:\n",
    "                    #If these two entities have the same stem, \n",
    "                    if entities[e].stem.lower() == entities[f].stem.lower():\n",
    "                        entities[f] = False\n",
    "                        entities[e] = Entity(entities[e].text, entities[e].lemma,\n",
    "                                             entities[e].stem, entities[e].first_appearance,\n",
    "                                             entities[e].kind, entities[e].count + 1)\n",
    "        \n",
    "    entities = list(filter(None, entities))\n",
    "    \n",
    "    return (sctext, entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWikidata(entities:list, limit:int=100):\n",
    "    ents = entities[:limit]\n",
    "    wikigeturl = \"https://www.wikidata.org/w/api.php?action=wbgetentities&sites=enwiki&languages=en&format=json&\"\n",
    "    \n",
    "    entdata = {}\n",
    "    while ents:\n",
    "        srch = '|'.join(e[0] for e in ents[:50])\n",
    "        del(ents[:50])\n",
    "        results = get(wikigeturl + urlencode({'titles': srch})).json()\n",
    "        entdata.update({k: v for k, v in results['entities'].items() if k[0] != '-'})\n",
    "        \n",
    "    outp = []\n",
    "    for idx, ent in entdata.items():\n",
    "        outp.append(EntityData(py_.get(ent, 'labels.en.value'),\n",
    "                                      py_.get(ent, 'descriptions.en.value'),\n",
    "                                      f'https://www.wikidata.org/wiki/{idx}',\n",
    "                                      py_.get(ent, 'claims.P625.0.mainsnak.datavalue.value.longitude', False),\n",
    "                                      py_.get(ent, 'claims.P625.0.mainsnak.datavalue.value.latitude', False)))\n",
    "        \n",
    "    return outp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createMap(paper_data, entity_data):\n",
    "    m = folium.Map()\n",
    "\n",
    "    paper_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "    for paper in paper_data:\n",
    "        folium.Marker(\n",
    "            location=[paper.coords[0], paper.coords[1]],\n",
    "            popup=f'<small>newspaper</small><h3>{paper.title}</h3>{paper.place}',\n",
    "            icon=folium.Icon(color='gray'),\n",
    "        ).add_to(paper_cluster)\n",
    "\n",
    "    entity_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "    for ent in entity_data:\n",
    "        if ent.lat and ent.long:\n",
    "            folium.Marker(\n",
    "                location=[ent.long, ent.lat],\n",
    "                popup=f'<small>named entity</small><h3>{ent.name}</h3>{ent.description}',\n",
    "                icon=folium.Icon(color='blue'),\n",
    "            ).add_to(entity_cluster)\n",
    "    \n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doAnalysis(btn):\n",
    "    progressbar.value = 0\n",
    "    statusWidget.clear_output()\n",
    "    entitiesbox.clear_output()\n",
    "    nlpbox.clear_output()\n",
    "    mapbox.clear_output()\n",
    "    \n",
    "    with statusWidget:\n",
    "        display(HTML('Getting data from <i>Chronicling America</i>'))\n",
    "        \n",
    "    inpt = searchCA(year1box.value,\n",
    "                     year2box.value,\n",
    "                     searchbox.value, \n",
    "                     resultsslider.value)\n",
    "    \n",
    "    with statusWidget:\n",
    "        display(HTML('Getting named entities and preparing per-page NLP statistics.'))\n",
    "    \n",
    "    progressbar.max = len(inpt)\n",
    "    \n",
    "    alltexts = []\n",
    "    allentities = []\n",
    "    frequencies = []\n",
    "    paperlocs = []\n",
    "    cntr = 0\n",
    "    for item in inpt:\n",
    "        cntr += 1\n",
    "        \n",
    "        pop = geocodecity(item['place_of_publication'])\n",
    "        if pop:\n",
    "            paperlocs.append(Paper(item['title'],\n",
    "                                  item['place_of_publication'], \n",
    "                                  pop))\n",
    "        \n",
    "        sctext, entities = getEntities(item)\n",
    "        alltexts.append(sctext)\n",
    "        allentities += entities\n",
    "        frequencies.append(Counter(word.lower() for word in nltk.word_tokenize(sctext) \n",
    "                                if not (word in sws or len(word) < 4)))\n",
    "\n",
    "        progressbar.value += 1\n",
    "    \n",
    "    with statusWidget:\n",
    "        display(HTML('Preparing search-wide NLP statistics.'))\n",
    "\n",
    "    corpustext = '\\n\\n'.join(alltexts)\n",
    "    corpusfreqs = Counter(word.lower() for word in nltk.word_tokenize(sctext) \n",
    "                                if not (word in sws or len(word) < 4))\n",
    "    corpuslen = len(nltk.word_tokenize(sctext))\n",
    "    corpusprobs = {k: v/corpuslen for k, v in corpusfreqs.items()}\n",
    "    upwords = {k: v - brownprobs.get(k, 1) for k, v in corpusprobs.items()}\n",
    "\n",
    "    #corpusfreqs = {k: (v, v / brownprobs.get(k, 1)) for (k, v) in corpusfreqs.items()}\n",
    "\n",
    "    for e in range(len(allentities)):\n",
    "        if allentities[e] != False:\n",
    "            for f in range(e+1, len(allentities)):\n",
    "                if allentities[f] != False:\n",
    "                    #If these two entities have the same stem, \n",
    "                    if allentities[e].stem.lower() == allentities[f].stem.lower():\n",
    "                        allentities[e] = Entity(allentities[e].text, allentities[e].lemma,\n",
    "                                             allentities[e].stem, allentities[e].first_appearance,\n",
    "                                             allentities[e].kind, allentities[e].count + allentities[f].count)\n",
    "                        allentities[f] = False\n",
    "\n",
    "    allentities = sorted(filter(None, allentities), key=attrgetter(\"count\"), reverse=True)\n",
    "    entitydata = getWikidata(allentities)\n",
    "    \n",
    "    fmap = createMap(paperlocs, entitydata)\n",
    "    \n",
    "    entdisplay = '\\n'.join([\n",
    "        f\"\"\"<p><b><a href=\"{ed.url}\" target=\"_blank\">{ed.name}</a></b>\n",
    "        <br/>{ed.description}</p>\"\"\"\n",
    "        for ed in entitydata\n",
    "    ])\n",
    "    nlpdisplay = '\\n'.join([\n",
    "       f\"<li><b>{x[0]}</b></li>\" for x in \n",
    "            sorted(upwords.items(), key=itemgetter(1), reverse=True)[:20]\n",
    "            if x[1] > .005])\n",
    "    \n",
    "    with entitiesbox:\n",
    "        display(HTML(entdisplay))\n",
    "    \n",
    "    with nlpbox:\n",
    "        display(HTML('<ul>'+nlpdisplay+'</ul>'))\n",
    "    \n",
    "    with mapbox:\n",
    "        display(fmap)\n",
    "    \n",
    "    with statusWidget:\n",
    "        display(HTML('<b>Done.</b>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1ae988c6194be6837d48c4e7adfe62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(BoundedIntText(value=1900, description='Start:', max=1963, min=1789), BoundedInt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resultsslider = widgets.IntSlider(\n",
    "    value=15,\n",
    "    min=1,\n",
    "    max=100,\n",
    "    step=1,\n",
    "    description='# of Results',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "year1box = widgets.BoundedIntText(\n",
    "    value=1900,\n",
    "    min=1789,\n",
    "    max=1963,\n",
    "    step=1,\n",
    "    description='Start:',\n",
    ")\n",
    "\n",
    "year2box = widgets.BoundedIntText(\n",
    "    value=1963,\n",
    "    min=1789,\n",
    "    max=1963,\n",
    "    step=1,\n",
    "    description='End:',\n",
    ")\n",
    "\n",
    "searchbox = widgets.Text(\n",
    "    placeholder='Enter search terms',\n",
    "    description='Search'\n",
    ")\n",
    "\n",
    "analyseButton = widgets.Button(\n",
    "    description='Go',\n",
    "    disabled=False,\n",
    "    tooltip='Run',\n",
    "    icon='' # (FontAwesome names without the `fa-` prefix)\n",
    ")\n",
    "\n",
    "analyseButton.on_click(doAnalysis)\n",
    "\n",
    "\n",
    "ui = widgets.HBox([widgets.VBox([year1box, year2box]), \n",
    "                   widgets.VBox([searchbox, resultsslider]), analyseButton])\n",
    "\n",
    "display(ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6701e3e9e18e4888b0d3179fb2f6572c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(layout=Layout(border='2px solid blue')), IntProgress(value=0, bar_style='success', descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.VBox([statusWidget, progressbar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80bf536e6bd1450f89f0e9e81b83ff16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GridBox(children=(Output(layout=Layout(border='1px solid black', grid_area='header')), Output(layout=Layout(bo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widgets.GridBox(children=[urlbox, entitiesbox, nlpbox, mapbox],\n",
    "        layout=widgets.Layout(\n",
    "            width='100%',\n",
    "            grid_template_rows='auto auto auto',\n",
    "            grid_template_columns='70% auto',\n",
    "            grid_gap='5px 5px',\n",
    "            grid_template_areas='''\n",
    "            \"header header\"\n",
    "            \"entities nlp\"\n",
    "            \"map map\"\n",
    "            ''')\n",
    "       )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
